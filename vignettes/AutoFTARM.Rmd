---
title: "AutoFTARM"
authors: "Sharon, Jason, Jayakrishna and Trilok"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{FTARM}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "left", comment = "#>")
```


`FTARM` (Fast Top-K Association Rule Mining) is a proposed algorithm that does association rule mining. Association rule mining is a branch of data mining that deals with identification of useful patterns within a given dataset. Compared with other existing algorithms that do association rule mining, `FTARM` is said to be more efficient. `FTARM` automates most of its procedures and, thus, it produces patterns with higher accuracy. `AutoFTARM` intends to further improve the accuracy of `FTARM` by fully automating the algorithm. 

Below is a demonstration of `FTARM's` performance compared to the performance of other algorithms that do association rule mining. As seen in figure 1 below, for different datasets (Pumsb and Connect), FTARM has better runtime and lower memory consumption compared to TopKrules and ETARM algorithms (Liu et al., 2021). Its better performance is owed to the fact that it produces much less rules that are more interpretable (Liu et al., 2021). 

![Figure 1: FTARM's Performance](C:/Users/skirw/Desktop/Academics/AutoFTRAM/man/figures/FTARM.png) 


## Significance of AutoFTARM

There is only one algorithm (`apriori`) in R programming language that does association rule mining. The `apriori` algorithm is found inside the `arules` package. With `apriori`, the user is required to provide the key parameters (`minimum support` and `minimum confidence`) that determine the number of rules (patterns) that are generated by the algorithm. This is a major drawback with `apriori`, as well as with the other existing algorithms, because it is quite tedious for the users to keep on adjusting the parameters till they find the optimum parameters that produces the best model that contains the desired set of rules. The proposed `FTARM` algorithm eliminates the need for the user to provide the minimum support value, however, the user is still required to provide the minimum confidence value. Therefore, our proposed `AutoFTARM` package intends to completely automate the FTARM algorithm by eliminating the need for the user to provide any of the key parameters used in association rule mining.   

## Demonstration of association rule mining

We used the existing `Apriori` algorithm to demonstrate association rule mining and to show how the `FTARM` algorithm is expected to work after its successful implementation.

```{r, message = FALSE, warning = FALSE}
# Load packages
library(arules)
library(knitr)
library(arulesViz)
library(dplyr)
library(RColorBrewer)
```

```{r}
# Load data
# The arules package comes with the 'Groceries' dataset.
data(Groceries)
```

```{r, warning=FALSE, message=FALSE, fig.width=6, fig.height=3}
# Generates the items that frequently appears in the dataset
# Reference (2)
itemFrequencyPlot(Groceries, topN = 15, type = "absolute", col = brewer.pal(8,'Dark2'))
```

As pointed out earlier, the minimum support and minimum confidence values are important, as they determine the number of rules generated. We used different sets of values to demonstrate the effect that the two input parameters have on the number of rules generated.

```{r, results = "hide"}
# Performing apriori with minimum support=0.002 and minimum confidence=0.05
# 10124 rules are generated.
model1 <- apriori(data = Groceries, 
                parameter = list(support=0.002, confidence=0.05))
```

```{r, results = "hide"}
# Performing apriori with minimum support=0.01 and minimum confidence=0.03
# 566 rules are generated
model2 <- apriori(data = Groceries, 
                parameter = list(support=0.01, confidence=0.03))
```

```{r, warning=FALSE, message=FALSE, fig.width=6, fig.height=3, fig.cap="Scatterplot of rules generated with minimum support of 0.002 and minimum confidence of 0.05"}
plot(model1, col = brewer.pal(12,'Paired'))
```

```{r, warning=FALSE, message=FALSE, fig.width=6, fig.height=3, fig.cap="Scatterplot of rules generated with minimum support of 0.01 and minimum confidence of 0.03"}
plot(model2, col = brewer.pal(12,'Paired'))
```


Interpreting and visualizing all generated rules can be quite challenging. As seen in the scatterplots above, there is a lot of overlap when visualizing a large number of rules. An interactive visualization does a better job of visualizing the rules. 

```{r, fig.width=6, fig.height=4}
# Reference(3)
# Interactive visualization of the rules.
sample_rules <- head(model2, n = 50, by = "confidence")

plot(sample_rules, method = "graph",  engine = "htmlwidget")
```


## Demonstration of AutoFTARM

Our success was dependent on the implementation and functioning of the `FTARM` algorithm. However, given that only the pseudo code for `FTARM` existed, we had to implement the algorithm from the ground up. `FTARM` has many building blocks. We implemented all the functions needed for `FTARM` to work, however, only a number of them were successful.

We used the [movie_lens](2.	https://grouplens.org/datasets/movielens/) dataset, which is publicly available. Only a subset of the data, with 400 observations was used. The sample data contains a list of movies that the users gave a rating of 5. The sample data was stored in a csv file named `movie_ratings.csv`, which is available on our [github](https://github.com/skirwa/AutoFTRAM) repository.

### Demonstration of FTARM's dependent functions that were successfully implemented:


```{r}
# Loading sample data
data_sample <- read.csv("C:/Users/skirw/Desktop/Academics/AutoFTRAM/movie_ratings.csv")
```

* `tids()` and `tidsForTwo()` - tids stands for transaction identifier of a set of items. These functions will be replaced in the future with one function that takes in a list of items, instead of taking in only one or two items. 

```{r}
# tids is the transaction identifier of a set of items.
# tids determine how frequently an item appears in the transactions.
tids <- function(searchFor, DB) {
  # movie_ratings implementation: count how many users watched a certain movie.
  DB <- as.list(DB)
  result <- lapply(DB, function(x) {
    sum(x == searchFor)
  })
  
  total <- 0
  for(i in result) {
    if(i[1] == 1) {
      total <- total + 1
    }
  }
  return(total)
}

# The outcome tells us that 9 users have watched the movie 'Toy Story' (with movie_id = 1). 
tids(1, data_sample)

```


```{r}
tidsForTwo <- function(searchFor1, searchFor2, DB) {
  # calculate the number of columns or transactions that contain x and y
  DB <- as.list(DB)
  result <- lapply(DB, function(x) sum(x == searchFor1) != 0 && sum(x == searchFor2) != 0)
  
  total <- 0
  for(i in result) {
    if(i[1]) {
      total <- total + 1
    }
  }
  return(total)
}

# The outcome tells us that 3 users have watched both 'Toy Story' (id = 1), and 'Grumpier' (id=3).
tidsForTwo(1, 3, data_sample)
```

* `sup()` - Used to calculate the support value of an item in the dataset. This function helps to generate rules (patterns).

```{r}
# Calculate the support value for an item.
# Support is the no of transactions (I) in DB with item i / total number of transactions in DB. 
sup <- function(DB, i) {
    
  support <- tids(i, DB) / length(DB)
  return(support)
}

# The support of the movie 'Shawshank Redemption' is 0.35
sup(data_sample, 318)
```

* `conf()` - Used to calculate the confidence level of an item in the given dataset. This function helps to generate rules (patterns).

```{r}
# Calculate the confidence value for an item
# Confidence is the no of transactions with items x and y / no of transactions with item x.
conf <- function(DB, x, y) {
    confidence = tidsForTwo(x, y, DB) / tids(x, DB)
    return (confidence)
}

# The confidence that a user will watch both the movies 'Toy Story' (id = 1) and 'Grumpier' (id=3) is 0.3333.
conf(data_sample, 1, 3)
```

* `minConf()` - Used to calculate the minimum confidence level. This function helps determine the number of rules generated. 

```{r}
# A list of items should be parsed into minConf and minSup
# However, we were unable to implement a tids function that takes in a list. It should be done in the future.
# Therefore, we used the tidsForTwo function instead.
minConf <- function(DB, searchFor1, searchFor2) {
  maxTid = -1
  x <- list()
  x <- append(x, searchFor1, searchFor2)
  for (i in x) {
    if (tids(i, DB) > maxTid) {
      maxTid = tids(i, DB)
    }
  }
  return(tidsForTwo(searchFor1, searchFor2, DB)/maxTid)
}

minConf(data_sample, 260, 318)
```

* `minSup()` - Used to compute the minimum support value. This function helps determine the number of rules generated.

```{r}
# The minSup function calculates the minimum support value.
minSup <- function(x, data) {
   return(tids(x, data)/length(data))
}

minSup(1, data_sample)
```

* `SortItems()` - Used to sort items in descending order based on their support values.

```{r}
SortItems <- function(DB) {
  # Define an empty vector
  itemsinData <- c()
  # Store the ids of the items in the database in the empty vector
  for (i in DB){
    itemsinData <- c(itemsinData, i)
  }
  # Convert the list of items to a dataframe
  new_df <- data.frame(sapply(itemsinData,c))
  # Rename the column with ids
  colnames(new_df)[1] ="id"
  # This function returns the support value for each item.
  func <- function(new_df){
    return(sapply(new_df$id, function(id) sum(new_df$id == id)/nrow(new_df)))
  }
  # Create a new column that contains the support values of the corresponding items.
  new_df$support <- func(new_df)
  # Sort the items in descending order based on their support values
  swapped <- new_df %>% arrange(desc(support))
  swapped
}

tail(SortItems(data_sample), 5)
```

### Project Aims

Aim 1: Develop a package (AutoFTARM) that improves the accuracy of FTARM algorithm.

* Successfully implemented some of FTARM’s dependent functions. AutoFTARM will be up and running once all the building blocks of FTARM are functioning.

Aim 2: Enhance Apriori algorithm by automating the computation of minimum support and minimum confidence values.

* Implemented functions that compute minimum support and minimum confidence. We plan on expanding the two functions, so that they can be adopted by the other algorithms that do association rule mining.


## References

1. Liu, X., Niu, X., & Fournier-Viger, P. (2021). Fast top-k association rule mining using rule generation property pruning. Applied Intelligence, 51(4), 2077-2093.

2. https://www.geeksforgeeks.org/apriori-algorithm-in-r-programming/

3. https://www.kirenz.com/post/2020-05-14-r-association-rule-mining/

